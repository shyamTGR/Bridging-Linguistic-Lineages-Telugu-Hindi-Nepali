{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":85416,"databundleVersionId":9690815,"sourceType":"competition"},{"sourceId":10437264,"sourceType":"datasetVersion","datasetId":6462423},{"sourceId":10472681,"sourceType":"datasetVersion","datasetId":6484427},{"sourceId":10472684,"sourceType":"datasetVersion","datasetId":6484430},{"sourceId":85986,"sourceType":"modelInstanceVersion","modelInstanceId":72246,"modelId":78150},{"sourceId":228542,"sourceType":"modelInstanceVersion","modelInstanceId":194881,"modelId":216781},{"sourceId":230504,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":196558,"modelId":218468}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Introduction: Bridging Linguistic Lineages with Sanskrit for Hindi and all the indian languages\n\nSanskrit is foundational in the development of modern Indo-Aryan languages. Sanskrit, as an ancient language, holds significant linguistic, cultural, and historical value, forming the root of many contemporary languages in the Indian subcontinent, including Hindi. By leveraging Sanskrit in the context of machine translation, we aim to bridge linguistic lineages and enhance the translation quality between languages like English and Hindi. Sanskrit not only provides a rich grammatical framework and vocabulary but also serves as a bridge that helps capture the nuances and deep structure inherent in the evolution of Hindi. This project explores the use of Sanskrit to improve linguistic accuracy, semantic alignment, and cultural context in machine translation, making it an essential tool for advancing our translation models and ensuring that modern language models respect historical and linguistic roots.\n\nThis model is initially only focused for hindi but can be adapted by leveraging Sanskrit’s linguistic structure, the model can better capture syntactic, semantic, and morphological nuances common across the **Indo-Aryan language family including including Hindi, Bengali, Marathi, Gujarati, Telugu, Tamil, Nepali and others**.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-15T02:01:17.146400Z","iopub.execute_input":"2025-01-15T02:01:17.146699Z","iopub.status.idle":"2025-01-15T02:01:17.493140Z","shell.execute_reply.started":"2025-01-15T02:01:17.146672Z","shell.execute_reply":"2025-01-15T02:01:17.492403Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/translations-supervised-data-for-fine-tuning-gemma/Translations_Multilingual.json\n/kaggle/input/gemma-language-tuning/submission_instructions.txt\n/kaggle/input/tokenized-data-for-finetuning/Tokenized_Translations.json\n/kaggle/input/kaggleinputsupervised-finetuned-weightsjax/jax/default/1/lora_weights_epoch1.lora.h5\n/kaggle/input/gemma2/keras/gemma2_instruct_2b_en/1/config.json\n/kaggle/input/gemma2/keras/gemma2_instruct_2b_en/1/tokenizer.json\n/kaggle/input/gemma2/keras/gemma2_instruct_2b_en/1/metadata.json\n/kaggle/input/gemma2/keras/gemma2_instruct_2b_en/1/model.weights.h5\n/kaggle/input/gemma2/keras/gemma2_instruct_2b_en/1/assets/tokenizer/vocabulary.spm\n/kaggle/input/supervised-finetuned-weights/jax/default/1/lora_weights_epoch3.lora.h5\n/kaggle/input/multilingual-text-corpus/multilingual_corpus_with_tags_reordered.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n!pip install -q -U keras-nlp datasets\n!pip install -q -U keras\n\nimport os\n\n# Set the backbend before importing Keras\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n# Avoid memory fragmentation on JAX backend.\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"\n\nimport keras_nlp\nimport keras\n\n# Run at half precision.\n#keras.config.set_floatx(\"bfloat16\")\n\n# Training Configurations\ntoken_limit = 1024\nlora_name = \"arya\"\nlora_rank = 4\nlr_value = 1e-4\ntrain_epoch = 20\nmodel_id = \"gemma2_instruct_2b_en\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T04:47:41.602515Z","iopub.execute_input":"2025-01-15T04:47:41.602827Z","iopub.status.idle":"2025-01-15T04:47:51.294496Z","shell.execute_reply.started":"2025-01-15T04:47:41.602800Z","shell.execute_reply":"2025-01-15T04:47:51.293375Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import keras\nimport keras_nlp\n\nimport time\n\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(model_id)\ngemma_lm.summary()\n\ntick_start = 0\n\ndef tick():\n    global tick_start\n    tick_start = time.time()\n\ndef tock():\n    print(f\"TOTAL TIME ELAPSED: {time.time() - tick_start:.2f}s\")\n\ndef text_gen(prompt):\n    tick()\n    input = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n    output = gemma_lm.generate(input, max_length=token_limit)\n    print(\"\\nGemma output:\")\n    print(output)\n    tock()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T04:47:15.129299Z","iopub.execute_input":"2025-01-15T04:47:15.129592Z","iopub.status.idle":"2025-01-15T04:47:23.763168Z","shell.execute_reply.started":"2025-01-15T04:47:15.129562Z","shell.execute_reply":"2025-01-15T04:47:23.761900Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-d26656440f59>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mgemma_lm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras_nlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGemmaCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_preset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mgemma_lm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model_id' is not defined"],"ename":"NameError","evalue":"name 'model_id' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"markdown","source":"# Running Intereference with the three languages before fine tuning","metadata":{}},{"cell_type":"code","source":"text_gen(\"వెళ్ళిపోతూ మళ్లీ వస్తానని అన్నాడు. కానీ, అతను తిరిగి రాలేదు. అతని గురించి ఏం అనిపిస్తోంది?\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text_gen(\"उसने कहा था कि वो लौटकर आएगा, लेकिन वो वापस नहीं आया। उसके बारे में आपको क्या लगता है?\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text_gen(\"उहाँले फर्किन्छु भन्नु भयो, तर उहाँ फर्किनु भएन। तपाईंलाई उहाँबारे के लाग्छ?\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Unsupervised Finetuning \nWe first performed unsupervised fine-tuning to enable the model to learn the underlying linguistic structures and patterns in both Sanskrit, Hindi and English without labeled data, This approach enhanced translation accuracy and strengthened the model's ability to bridge the linguistic connection between Sanskrit and Hindi.\n\n*The text corpus was primarily sourced from the **OPUS project** and then processed to eliminate unwanted characters, tags, and noise.*","metadata":{}},{"cell_type":"code","source":"import keras\nimport keras_nlp\nfrom datasets import load_dataset\n\n# Load Gemma tokenizer\nmodel_id = \"gemma2_instruct_2b_en\"\ntokenizer = keras_nlp.models.GemmaTokenizer.from_preset(model_id)\n\n# Configuration\ntoken_limit = 256  # Maximum token length\nnum_data_limit = 1000  # Limit on the number of examples to process\n\n# Language tags mapping\nlanguage_tags = {\n     \"san\":\"sanskrit\",\n    \"tel\":\"telugu\",\n    \"hin\":\"hindi\",\n    \"npi\":\"nepali\"\n}\n\n# Load dataset\ndataset_path = \"/kaggle/input/multilingual-text-corpus/multilingual_corpus_with_tags_reordered.txt\"\nraw_dataset = load_dataset(\"text\", data_files={\"train\": dataset_path})\n\n# Prepare dataset for fine-tuning\ntrain_data = []\n\n# Loop through the dataset and tokenize\nfor example in raw_dataset[\"train\"]:\n    text = example[\"text\"]\n    \n    # Extract the language tag (example assumes the language is in the first part of the text)\n    # Example: \"<tel> This is a Telugu sentence.\"\n    language = text.split(\">\")[0][1:]  # Extract \"tel\" from \"<tel>\"\n    tag = language_tags.get(language, \"<unk>\")  # Use <unk> for unknown languages\n    #print(language)\n    # Add language tag explicitly\n    tagged_text = f\"{tag} {text}\"\n\n    # Tokenize the text\n    tokenized = tokenizer.tokenize(tagged_text)  # Tokenize the tagged text\n    token_length = len(tokenized)  # Get the length of the tokenized sequence\n    \n    # Filter long sequences and add to training data\n    if token_length < token_limit:\n        train_data.append(tagged_text)\n    \n\n\n# Output dataset stats and examples\nprint(f\"Number of training examples: {len(train_data)}\")\nprint(f\"First example:\\n{train_data[0]}\")\nprint(f\"Second example:\\n{train_data[1]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T08:40:06.892914Z","iopub.execute_input":"2025-01-11T08:40:06.893245Z","iopub.status.idle":"2025-01-11T08:42:51.163705Z","shell.execute_reply.started":"2025-01-11T08:40:06.893220Z","shell.execute_reply":"2025-01-11T08:42:51.162894Z"}},"outputs":[{"name":"stdout","text":"Number of training examples: 97573\nFirst example:\nsanskrit <san> स्वदेहे चेल्लिखितवान्वचनमनवगम्यं तेनाप्यचिन्तिते फल उपलब्धे किं कर्तव्यम् ?\nSecond example:\nsanskrit <san> तत्त्वमसि ।\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Enable LoRA (Low-Rank Adaptation)\nlora_rank = 4  # LoRA rank\ngemma_lm.backbone.enable_lora(rank=lora_rank)\ngemma_lm.preprocessor.sequence_length = token_limit\n\n# Configure the optimizer\noptimizer = keras.optimizers.AdamW(\n    learning_rate=lr_value,\n    weight_decay=0.01,\n)\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T08:43:56.894385Z","iopub.execute_input":"2025-01-11T08:43:56.894691Z","iopub.status.idle":"2025-01-11T08:43:56.903592Z","shell.execute_reply.started":"2025-01-11T08:43:56.894665Z","shell.execute_reply":"2025-01-11T08:43:56.902688Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class SaveLoRAWeightsCallback(keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        lora_weights_path = f\"/kaggle/working/lora_weights_epoch{epoch + 1}.lora.h5\"\n        gemma_lm.backbone.save_lora_weights(lora_weights_path)\n        print(f\"Saved LoRA weights to: {lora_weights_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T08:43:59.160288Z","iopub.execute_input":"2025-01-11T08:43:59.160643Z","iopub.status.idle":"2025-01-11T08:43:59.165463Z","shell.execute_reply.started":"2025-01-11T08:43:59.160599Z","shell.execute_reply":"2025-01-11T08:43:59.164549Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Fine-tune the model\nepochs = 10\nbatch_size = 8\n\ngemma_lm.fit(\n    train_data,  # The tokenized dataset\n    epochs=epochs,\n    batch_size=batch_size,\n    callbacks=[SaveLoRAWeightsCallback()],\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Supervised Fine-tuning\nIn the next phase, we applied supervised fine-tuning to the already unsupervised fine-tuned model, using labeled data to further refine its performance and enhance translation accuracy between English and Hindi.\n\n*The text corpus was primarily sourced from the **OPUS project in multiple languages, then mapped between them.** This approach is preferred over using a translation dataset, as both datasets are generated in the original languages rather than being translated. The corpus was then processed to eliminate unwanted characters, tags, and noise.*","metadata":{}},{"cell_type":"code","source":"\nimport keras_nlp\nimport keras\n\n\n# Load Gemma 2 model\nmodel_id = \"gemma2_instruct_2b_en\"\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(model_id)\n\n# Enable LoRA and load weights\nlora_rank = 4\ngemma_lm.backbone.enable_lora(rank=lora_rank)\nlora_weights_path = \"/kaggle/input/kaggleinputsupervised-finetuned-weightsjax/jax/default/1/lora_weights_epoch1.lora.h5\"\ngemma_lm.backbone.load_lora_weights(lora_weights_path)\n\nprint(\"LoRA weights loaded successfully.\")\ngemma_lm.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T02:04:43.401394Z","iopub.execute_input":"2025-01-15T02:04:43.401593Z","iopub.status.idle":"2025-01-15T02:05:45.742108Z","shell.execute_reply.started":"2025-01-15T02:04:43.401574Z","shell.execute_reply":"2025-01-15T02:05:45.741393Z"}},"outputs":[{"name":"stdout","text":"LoRA weights loaded successfully.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,617,270,528\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,617,270,528\u001b[0m (9.75 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> (9.75 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,928,640\u001b[0m (11.17 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,928,640</span> (11.17 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n</pre>\n"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import json\n\n# Load translation dataset\njson_path = \"/content/drive/MyDrive/Translations_Multilingual.json\"\nwith open(json_path, \"r\", encoding=\"utf-8\") as f:\n    translation_data = json.load(f)\n\n# Configuration\ntoken_limit = 1024\ntrain_data = []\n\n# Prepare data for fine-tuning\nfor example in translation_data[0]: \n    prompt = example[\"prompt\"]\n    response = example[\"response\"]\n    #print(f\"Prompt: {prompt}\")\n\n    # Prepare input-output text format for supervised learning\n    input_text = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n{response}<end_of_turn>\"\n\n    # Tokenize the text using the preprocessor\n    tokenized = gemma_lm.preprocessor(input_text)  # Returns a tuple\n\n    # Extract token_ids and attention mask\n    token_ids = tokenized[0][\"token_ids\"]\n    #print(f\"Token IDs: {token_ids.numpy()}\")\n    #print(len(token_ids))\n    # Filter long sequences based on token length\n    if len(token_ids) <= token_limit:\n        train_data.append(input_text)\n\n# Display dataset stats\nprint(f\"Number of training examples: {len(train_data)}\")\nif len(train_data) > 0:\n    print(f\"First example:\\n{train_data[0]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\n# Save tokenized training data\ntokenized_data_path = \"/content/drive/MyDrive/Tokenized_Translations.json\"\nwith open(tokenized_data_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(train_data, f, ensure_ascii=False, indent=4)\n\nprint(f\"Tokenized data saved to: {tokenized_data_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\ntokenized_data_path = \"/content/drive/MyDrive/Tokenized_Translations.json\"\nwith open(tokenized_data_path, \"r\", encoding=\"utf-8\") as f:\n    train_data = json.load(f)\n\nprint(f\"Tokenized data loaded successfully. Number of examples: {len(train_data)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T12:00:59.008206Z","iopub.execute_input":"2025-01-13T12:00:59.008801Z","iopub.status.idle":"2025-01-13T12:00:59.013608Z","shell.execute_reply.started":"2025-01-13T12:00:59.008770Z","shell.execute_reply":"2025-01-13T12:00:59.012680Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from keras.mixed_precision import set_global_policy\nset_global_policy(\"mixed_float16\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T02:58:55.214531Z","iopub.execute_input":"2025-01-15T02:58:55.214829Z","iopub.status.idle":"2025-01-15T02:58:55.219462Z","shell.execute_reply.started":"2025-01-15T02:58:55.214808Z","shell.execute_reply":"2025-01-15T02:58:55.218584Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Running Interference","metadata":{}},{"cell_type":"code","source":"\nimport keras_nlp\nimport keras\n\n\n# Load Gemma 2 model\nmodel_id = \"gemma2_instruct_2b_en\"\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(model_id)\n\n# Enable LoRA and load weights\nlora_rank = 4\ngemma_lm.backbone.enable_lora(rank=lora_rank)\nlora_weights_path = \"/kaggle/input/kaggleinputsupervised-finetuned-weightsjax/jax/default/3/lora_weights_epoch4.lora.h5\"\ngemma_lm.backbone.load_lora_weights(lora_weights_path)\n\nprint(\"LoRA weights loaded successfully.\")\ngemma_lm.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T05:42:06.304810Z","iopub.execute_input":"2025-01-15T05:42:06.305103Z","iopub.status.idle":"2025-01-15T05:43:06.675665Z","shell.execute_reply.started":"2025-01-15T05:42:06.305082Z","shell.execute_reply":"2025-01-15T05:43:06.674862Z"}},"outputs":[{"name":"stdout","text":"LoRA weights loaded successfully.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,617,270,528\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,617,270,528\u001b[0m (9.75 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> (9.75 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,928,640\u001b[0m (11.17 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,928,640</span> (11.17 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n</pre>\n"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import keras\nimport keras_nlp\n\nimport time\n\n\ntick_start = 0\ntoken_limit=1024\ndef tick():\n    global tick_start\n    tick_start = time.time()\n\ndef tock():\n    print(f\"TOTAL TIME ELAPSED: {time.time() - tick_start:.2f}s\")\n\ndef text_gen(prompt):\n    tick()\n    input = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n    output = gemma_lm.generate(input, max_length=token_limit)\n    print(\"\\nGemma output:\")\n    print(output)\n    tock()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T05:43:34.226612Z","iopub.execute_input":"2025-01-15T05:43:34.226923Z","iopub.status.idle":"2025-01-15T05:43:34.231737Z","shell.execute_reply.started":"2025-01-15T05:43:34.226899Z","shell.execute_reply":"2025-01-15T05:43:34.230822Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"text_gen(\"Translate to Hindi: The ship rested on the mountains of Ararat on the seventeenth day of the seventh month.\")\ntext_gen(\"Translate to Hindi: The waters increased greatly on the earth, so that all the high mountains under the whole sky were covered.\")\ntext_gen(\"Translate to Hindi: All who had the breath of life in their nostrils, of all that was on the dry land, died.\")\ntext_gen(\"Translate to Hindi: It happened at the end of forty days, that Noah opened the window of the ship which he had made and sent out a raven.\")\ntext_gen(\"Translate to Hindi: Then the dove came back to him in the evening, and behold, in her mouth was an olive leaf.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T05:47:26.588074Z","iopub.execute_input":"2025-01-15T05:47:26.588382Z","iopub.status.idle":"2025-01-15T05:47:26.593342Z","shell.execute_reply.started":"2025-01-15T05:47:26.588360Z","shell.execute_reply":"2025-01-15T05:47:26.592613Z"}},"outputs":[{"name":"stdout","text":"Gemma output:\n<start_of_turn>user\nTranslate to Hindi: The ship rested on the mountains of Ararat on the seventeenth day of the seventh month.\n<end_of_turn>\n<start_of_turn>model\nसातवें महीने के सत्रहवें दिन, जहाज अरारात के पहाड़ों पर विश्राम कर रहा था।\n<end_of_turn>\nTOTAL TIME ELAPSED: 1.89s\n\nGemma output:\n<start_of_turn>user\nTranslate to Hindi: The waters increased greatly on the earth, so that all the high mountains under the whole sky were covered.\n<end_of_turn>\n<start_of_turn>model\nपृथ्वी पर पानी अत्यधिक बढ़ गया, जिससे आकाश के नीचे सभी ऊँचे पहाड़ ढक गए।\n<end_of_turn>\nTOTAL TIME ELAPSED: 1.73s\n\nGemma output:\n<start_of_turn>user\nTranslate to Hindi: All who had the breath of life in their nostrils, of all that was on the dry land, died.\n<end_of_turn>\n<start_of_turn>model\nजो जीवित प्राणों की साँस अपने नथुनों से ले रहे थे, जो भूमि पर थे, वे सभी मर गए।\n<end_of_turn>\nTOTAL TIME ELAPSED: 1.80s\n\nGemma output:\n<start_of_turn>user\nTranslate to Hindi: It happened at the end of forty days, that Noah opened the window of the ship which he had made and sent out a raven.\n<end_of_turn>\n<start_of_turn>model\nचालीस दिनों के अंत में, नूह ने उस जहाज की खिड़की खोली जिसे उसने बनाया था, और एक कौआ बाहर भेजा।\n<end_of_turn>\nTOTAL TIME ELAPSED: 2.13s\n\nGemma output:\n<start_of_turn>user\nTranslate to Hindi: Then the dove came back to him in the evening, and behold, in her mouth was an olive leaf.\n<end_of_turn>\n<start_of_turn>model\nफिर कबूतर शाम को वापस आया, और देखा, उसके मुँह में एक जैतून का पत्ता था।\n<end_of_turn>\nTOTAL TIME ELAPSED: 1.94s\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"text_gen(\"Translate to English: और शुद्ध, और अशुद्ध दोनो प्रकार के पशुओं में से, पक्षियों,\")\ntext_gen(\"Translate to English: वह लड़का अपनी किताबें पढ़ रहा है।\")\ntext_gen(\"Translate to English: मुझे तुम्हारी मदद की ज़रूरत है।\")\ntext_gen(\"Translate to English: वह हर सुबह दौड़ने जाता है।\")\ntext_gen(\"Translate to English: यह एक सुंदर दिन है।\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T05:52:10.072455Z","iopub.execute_input":"2025-01-15T05:52:10.072881Z","iopub.status.idle":"2025-01-15T05:52:10.078378Z","shell.execute_reply.started":"2025-01-15T05:52:10.072850Z","shell.execute_reply":"2025-01-15T05:52:10.077463Z"}},"outputs":[{"name":"stdout","text":"Gemma output:\n<start_of_turn>user\nTranslate to English: और शुद्ध, और अशुद्ध दोनो प्रकार के पशुओं में से, पक्षियों,<end_of_turn>\n<start_of_turn>model\nAnd of the clean and unclean, of the animals, and of the birds,<end_of_turn>\nTOTAL TIME ELAPSED: 1.42s\n\nGemma output:\n<start_of_turn>user\nTranslate to English: वह लड़का अपनी किताबें पढ़ रहा है।<end_of_turn>\n<start_of_turn>model\nThe boy is reading his books.<end_of_turn>\nTOTAL TIME ELAPSED: 1.35s\n\nGemma output:\n<start_of_turn>user\nTranslate to English: मुझे तुम्हारी मदद की ज़रूरत है।<end_of_turn>\n<start_of_turn>model\nI need your help.<end_of_turn>\nTOTAL TIME ELAPSED: 1.29s\n\nGemma output:\n<start_of_turn>user\nTranslate to English: वह हर सुबह दौड़ने जाता है।<end_of_turn>\n<start_of_turn>model\nHe goes for a run every morning.<end_of_turn>\nTOTAL TIME ELAPSED: 1.50s\n\nGemma output:\n<start_of_turn>user\nTranslate to English: यह एक सुंदर दिन है।<end_of_turn>\n<start_of_turn>model\nIt is a beautiful day.<end_of_turn>\nTOTAL TIME ELAPSED: 1.20s\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Model Evaluations ","metadata":{}},{"cell_type":"code","source":"import json\n\n# Load tokenized data\ntokenized_data_path = \"/kaggle/input/tokenized-data-for-finetuning/Tokenized_Translations.json\"\nwith open(tokenized_data_path, \"r\", encoding=\"utf-8\") as f:\n    tokenized_data = json.load(f)\n\nprint(f\"Loaded {len(tokenized_data)} tokenized examples.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T02:06:02.347740Z","iopub.execute_input":"2025-01-15T02:06:02.348106Z","iopub.status.idle":"2025-01-15T02:06:02.842053Z","shell.execute_reply.started":"2025-01-15T02:06:02.348080Z","shell.execute_reply":"2025-01-15T02:06:02.841242Z"}},"outputs":[{"name":"stdout","text":"Loaded 61541 tokenized examples.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# BLEU\nBLEU scores range from 0 to 1, where 1 indicates perfect overlap between the generated text and the reference (ground truth) text.\n* 0.0 - 0.2: Low performance. \n* 0.2 - 0.3: Mediocre performance.\n* 0.3 - 0.4: Good performance. \n* 0.4 - 0.5: Very good performance. T\n* 0.5+: Excellent performance.","metadata":{}},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\n\ndef evaluate_model(model, tokenized_data, tokenizer):\n    bleu_scores = []\n    \n    for example in tokenized_data[:10000]:\n        # Prepare the input text\n        input_text = example  # The tokenized input\n        expected_response = example.split(\"<start_of_turn>model\\n\")[-1].strip(\"<end_of_turn>\")\n        \n        # Generate model prediction\n        prediction = model.generate(input_text, max_length=1024)\n        \n        # Post-process prediction\n        prediction_text = prediction.replace(\"<start_of_turn>model\\n\", \"\").strip(\"<end_of_turn>\")\n        \n        # Calculate BLEU score\n        bleu_score = sentence_bleu([expected_response.split()], prediction_text.split())\n        bleu_scores.append(bleu_score)\n    \n    # Average BLEU score across examples\n    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n    return avg_bleu, bleu_scores\n\navg_bleu, bleu_scores = evaluate_model(gemma_lm, tokenized_data, gemma_lm.preprocessor.tokenizer)\n\nprint(f\"Average BLEU Score: {avg_bleu}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T02:14:14.601070Z","iopub.execute_input":"2025-01-15T02:14:14.601364Z","iopub.status.idle":"2025-01-15T02:14:14.606051Z","shell.execute_reply.started":"2025-01-15T02:14:14.601344Z","shell.execute_reply":"2025-01-15T02:14:14.605105Z"}},"outputs":[{"name":"stdout","text":"Average BLEU Score: 0.8578588157079955\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"> Achieved Average BLEU Score ( Range [0-1] ): **0.8578588157079955**\n* 0.0 - 0.2: Low performance. \n* 0.2 - 0.3: Mediocre performance.\n* 0.3 - 0.4: Good performance. \n* 0.4 - 0.5: Very good performance. T\n* 0.5+: Excellent performance.","metadata":{}},{"cell_type":"markdown","source":"# Perplexity (PPL)\n* Lower Perplexity: Indicates the model's predictions are closer to the ground truth.\n* Higher Perplexity: Indicates the model is less confident and worse at predicting the next word.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\n\ndef calculate_perplexity(model, data, tokenizer):\n    total_loss = 0\n    total_tokens = 0\n\n    for example in data:\n        input_text = example  # tokenized input text\n        target_text = example.split(\"<start_of_turn>model\\n\")[-1].strip(\"<end_of_turn>\")\n\n        # Tokenize the input and target text using Gemma's tokenizer (adjust this based on the tokenizer method)\n        input_ids = tokenizer.tokenize(input_text)  # Adjust this if needed\n        target_ids = tokenizer.tokenize(target_text)  # Adjust this if needed\n\n        # Create the attention mask (1 for valid tokens, 0 for padding tokens)\n        attention_mask = [1] * len(input_ids)  # Assuming no padding, you can add logic for padding if needed\n        target_attention_mask = [1] * len(target_ids)  # Similarly for target sequence\n\n        # Convert to tensors\n        input_tensor = tf.convert_to_tensor([input_ids])\n        target_tensor = tf.convert_to_tensor([target_ids])\n        attention_mask_tensor = tf.convert_to_tensor([attention_mask])\n\n        # Generate predictions (log-probabilities)\n        logits = model([input_tensor, attention_mask_tensor], training=False)  # Provide input and attention mask\n\n        # Check the shape of logits and target tensor\n        #print(f\"Logits shape: {logits.shape}, Target shape: {target_tensor.shape}\")\n\n        # Padding target tensor to match logits shape (model expects logits with seq_len of 46)\n        target_len = logits.shape[1]  # Get model's output sequence length\n        target_tensor_padded = tf.pad(target_tensor, [[0, 0], [0, target_len - target_tensor.shape[1]]], constant_values=0)\n\n        # Compute the loss (negative log likelihood)\n        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='sum_over_batch_size')  # Use sum_over_batch_size for averaging loss\n        loss = loss_fn(target_tensor_padded, logits)\n\n        # Accumulate loss and token count\n        total_loss += loss.numpy()  # Loss will be averaged across the sequence length\n        total_tokens += target_len\n\n    # Calculate Perplexity (average loss per token)\n    perplexity = np.exp(total_loss / total_tokens)  # Perplexity is calculated as exp(loss per token)\n    return perplexity\n\n# Assuming you have your tokenized data and model ready\nperplexity = calculate_perplexity(gemma_lm, tokenized_data[:1000], gemma_lm.preprocessor.tokenizer)\nprint(f\"Perplexity: {perplexity}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T02:24:01.071513Z","iopub.execute_input":"2025-01-15T02:24:01.071842Z","iopub.status.idle":"2025-01-15T02:24:43.892993Z","shell.execute_reply.started":"2025-01-15T02:24:01.071818Z","shell.execute_reply":"2025-01-15T02:24:43.892164Z"}},"outputs":[{"name":"stdout","text":"Perplexity: 1.268300427793318\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"> Achieved Perplexity ( Range[1-10000] ): **1.268300427793318**\n* Lower Perplexity: Indicates the model's predictions are closer to the ground truth.\n* Higher Perplexity: Indicates the model is less confident and worse at predicting the next word.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}